<!DOCTYPE html>
<html lang="en">
<html>
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="../static/styles.css">
        <title>TXTRNZ</title>
    </head>
    <body>
    <header>
        <p>Text-Only Version <a class="full-version-link button" href="https://www.rnz.co.nz/news/national/530188/psychologist-sounds-alarm-on-use-of-artificial-intelligence-in-mental-health-apps">Go to article page</a></p>
    </header>
    <h1><a href="/">TXTRNZ</a></h1>
        <div class="hr-line"></div>
        <h2>Is AI the answer to our mental health crisis?</h2>
        <p>
Millions of people are turning to mental health apps for help - with some boasting artificial-intelligence chatbots to help people work through their problems. Audio
</p>
        
            <p>
Millions of people are turning to mental health apps for help, but psychologist Louise Cowpertwait is worried not all are clinically safe or user-friendly (file image). 
Photo: RNZ / Rebekah Parsons-King
</p>
        
            <p>You might have consulted Dr Google over a health issue, but how would you feel about talking to an AI chatbot about your mental health?</p>
        
            <p>Millions of people are turning to mental health apps for help - with some boasting artificial-intelligence chatbots to help people work through their problems.</p>
        
            <p>But one psychologist was worried not all of these apps were clinically safe or user-friendly - meaning people might give up on them and miss out on the help they needed.</p>
        
            <p>Clinical psychologist and MindMatters chief executive Louise Cowpertwait spoke to Checkpoint about putting mental health app Wysa's AI chatbot feature to the test.</p>
        
            <p>Cowpertwait asked it what she should do if she thought she had ADHD.</p>
        
            <p>"Daily conversations with me will help break cycles you may be stuck in," the bot responded.</p>
        
            <p>"It's kind of assuming that someone with ADHD are stuck in cycles, which actually is pretty invalidating," she observed.</p>
        
            <p>
Mental health app Wysa has an AI chatbot feature that responds to questions about issues they are facing. 
Photo: Luka Forman
</p>
        
            <p>It then followed up with more advice.</p>
        
            <p>"If I told you you were enough, just as you are, would you believe me?"</p>
        
            <p>"That's a bit weird - see, it's just feeling like not actually relevant; like it's not listening to me. I'm starting to get quite annoyed with it already."</p>
        
            <p>Cowpertwait then asked the chatbot how she could get a diagnosis for ADHD.</p>
        
            <p>"For a minute, imagine you felt completely confident and accepted: tell me the details of how you would be, how others would treat you," it responded.</p>
        
            <p>"So at this point I'm bored and I'm going to leave," she said.</p>
        
            <p>Wysa's website said it had provided help for 6 million people across 95 countries, and 91 percent of its users found it helpful.</p>
        
            <p>It was also part of the NHS mental health pathway in the UK, helping to triage patients and support waitlists.</p>
        
            <p>The app was clinically tested - but Cowpertwait said its user experience fell short, which could make it invalidating for people using it.</p>
        
            <p>"Best-case scenario is it's just a bit annoying or frustrating - and then you bail on the product and stop using it.</p>
        
            <p>"The other risk is that you might go hunting for a product that feels like a better fit and end up engaging with something unsafe."</p>
        
            <p>A 2019 study found just 4 percent of users who downloaded a mental health app continued to use it after 15 days, dropping to 3 percent after 30 days.</p>
        
            <p>AI technology created an opportunity to make mental health support more accessible and cheaper, Cowpertwait said, freeing up clinicians for more complex cases.</p>
        
            <p>"So you could have a product that basically talks to you, validates how you're feeling and then connects you to what you can do to access the right support - whether that's providing some therapy through the product, or where to go externally."</p>
        
            <p>But Cowpertwait did not think AI was good enough to provide that advice just yet.</p>
        
            <p>College of General Practitioners medical director Luke Bradford said its members did not usually recommend AI-based apps, but patients would sometimes mention to their GP that they were using them.</p>
        
            <p>They could be a useful tool - if they were used in the right way, Bradford said.</p>
        
            <p>"There's no harm in patients finding the support through apps - they just need to know when to seek further help if things are really struggling."</p>
        
            <p>GPs did refer patients to online tools like New Zealand-based Just A Thought - a website providing online courses on mental illnesses, but did not use AI, he said.</p>
        
            <p>Checkpoint contacted Wysa for comment.</p>
        
            <p>If it is an emergency and you feel like you or someone else is at risk, call 111.</p>
        
        
        <div class="hr-line"></div>
        <br>
        <footer>
            <nav class="lower-nav-container">
              <li><a href="https://tom.so/experiment/txtrnz">About this site</a></li>
              <li><a href="https://www.rnz.co.nz/about">About RNZ</a></li>
            </nav>
          </footer>
    </body>
</html>