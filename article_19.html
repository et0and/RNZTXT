<!DOCTYPE html>
<html lang="en">
<html>
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="../static/styles.css">
        <title>TXTRNZ</title>
    </head>
    <body>
    <header>
        <p>Text-Only Version <a class="full-version-link button" href="https://www.rnz.co.nz/news/on-the-inside/542286/online-violence-and-misogyny-are-still-on-the-rise-nz-needs-a-tougher-response">Go to article page</a></p>
    </header>
    <h1><a href="/">TXTRNZ</a></h1>
        <div class="hr-line"></div>
        <h2>Online violence and misogyny on the rise - NZ needs tougher response</h2>
        <p>
The Human Rights commission's claim the Code of Practice for Online Safety and Harms was not fit for purpose apparently drew a sharp legal response from theâ€¦
</p>
        
            <p>By Cassandra Mudgway of </p>
        
            <p>
Photo: 123rf
</p>
        
            <p>Yesterday's revelation of a 2023 standoff between the Human Rights Commission and New Zealand's internet safety agencies highlights lingering concern about the current online safety code.</p>
        
            <p>According to the report from RNZ, the commission told NZ Tech and Netsafe that social media companies X Corp. and Meta failed to protect former prime minister Jacinda Ardern from misogynistic and dehumanising violence across their platforms.</p>
        
            <p>The commission's claim that the Code of Practice for Online Safety and Harms was not fit for purpose apparently drew a sharp legal response from the agencies, which argued the commission showed bias and had overstepped its remit.</p>
        
            <p>But the historical incident raises important questions New Zealand has yet to grapple with properly.</p>
        
            <p>Established in 2022, the code is a voluntary set of commitments co-designed with the technology industry, including some social media companies such as Meta and X-Corp.</p>
        
            <p>Companies become signatories to the code and agree to its commitments. The current signatories are Meta, Google, TikTok, Twitch and X Corp.</p>
        
            <p>
Apps for social media services including Facebook, X (formerly Twitter), Instagram, Threads, Telegram and others. 
Photo: Victor Okhrimets / 123RF
</p>
        
            <p>Among other provisions, the code asks signatories to take steps to reduce harmful content on their platforms or services, including harassment (where there is an intent to cause harm), hate speech (which includes sexist hate speech), incitement of violence and disinformation.</p>
        
            <p>The code is not legally enforceable. Compliance relies on willingness to adopt such measures. But there is an accountability structure in the form of an oversight committee. The public can lodge complaints with the committee if they believe signatories have breached the code, and the committee can remove a signatory from the code.</p>
        
            <p>When it was launched, the code received some international acclaim as an example of best practice for digital safety. But its critics argued that because it was co-written with social media companies, the commitments were not as strong or effective as they might have been.</p>
        
            <p>Last year, Netsafe rang the alarm about increasing rates of online misogyny and violent extremism, including the targeting of public figures and politicians.</p>
        
            <p>This raises obvious questions about the code's effectiveness. Since the Human Rights Commission cited the extreme online violence directed at Jacinda Ardern, former Green Party MP Golriz Ghahraman has spoken about the violent online misogyny and racism she experienced while in office.</p>
        
            <p>
Jacinda Ardern gives her valedictory speech to a packed debating chamber at Parliament. 
Photo: Phil Smith
</p>
        
            <p>These forms of gender-based violence are a breach of women's human rights. They also lead to women politicians self-censoring, avoiding social media, and generally having less contact with the public.</p>
        
            <p>Some overseas studies have shown prolonged exposure to online violence has led to women MPs leaving office sooner than planned. Overall, online harm endangers representative democracy and breaches women's rights to participate in politics.</p>
        
            <p>The human rights implications also mean the New Zealand government has legal duties under international treaties to prevent online gender-based violence.</p>
        
            <p>The United Nations has also called on social media companies to do more to prevent the spread of racial hatred. As such, it is a function of the Human Rights Commission to promote and monitor compliance with international standards.</p>
        
            <p>In its current form, the code is not effective. Its commitments aim to reduce harm rather than eliminate it, and it is not comprehensive about the kinds of harm it wants signatories to reduce.</p>
        
            <p>For example, it does not include reference to "volumetric" attacks - the type of coordinated harassment campaigns against a person that were directed at Ardern.</p>
        
            <p>Further, the code's threshold for "harm" is high, requiring the online violence to pose an imminent and serious threat to users' safety. This does not easily capture the types of gender-based violence, such as misogynistic hate speech, that over time normalise violence against women.</p>
        
            <p>The code also emphasises the role of users in managing harmful content, rather than placing a responsibility on the platforms to investigate how their services and technologies might be misused to cause harm.</p>
        
            <p>Relying on voluntary commitments also puts New Zealand out of step with other countries such as the United Kingdom and Australia which have legally enforceable requirements for social media companies to protect online safety.</p>
        
            <p>Placing that burden on users - to block, report or remove content - is merely reactive. It does not prevent harm because it has already happened. And for some groups, such as MPs and public figures, the harm they receive can be overwhelming and seemingly endless.</p>
        
            <p>Preventing online gender-based violence requires proactive measures that are legally enforceable. To fulfil its international obligations, the government should urgently review the need for legal regulation that places the burden of online safety on large social media companies rather than on users.</p>
        
            <p>This story was originally published on The Conversation.</p>
        
        
        <div class="hr-line"></div>
        <br>
        <footer>
            <nav class="lower-nav-container">
              <li><a href="https://tom.so/experiment/txtrnz">About this site</a></li>
              <li><a href="https://www.rnz.co.nz/about">About RNZ</a></li>
            </nav>
          </footer>
    </body>
</html>